---

# Default compose file for development and production.
# Should be used directly in development.
# Automatically loads `docker-compose.override.yml` if it exists.
# No extra steps required.
# Should be used together with `docker/docker-compose.prod.yml`
# in production.

version: "3.6"
services:
  db:
    image: "postgres:15-alpine"
    restart: unless-stopped
    volumes:
      - pgdata:/var/lib/postgresql/data
    networks:
      - webnet
    env_file: ./config/.env

  frontend:
    image: afaneor/lct-yakutia-frontend:latest
    restart: unless-stopped
    networks:
      - webnet

  web:
    <<: &web
      # Image name is changed in production:
      image: "afaneor/lct-yakutia-backend"
      build:
        target: production_build
        context: .
        dockerfile: ./docker/django/Dockerfile
        args:
          DJANGO_ENV: development
          NOVA_BUILD_ID: "${CI_PIPELINE_IID:-No CI}"
          NOVA_RELEASE: "${CI_COMMIT_TAG:-No release image}"
          NOVA_PIPELINE_URL: "${CI_PIPELINE_URL:-No CI}"
      platform: linux/amd64
      volumes:
        - django-static:/var/www/django/static
      depends_on:
        - db
      networks:
        - webnet
      env_file: ./config/.env
      environment:
        DJANGO_DATABASE_HOST: db

    command: start-web-server
    healthcheck:
      # We use `$$` here because:
      # one `$` goes to shell,
      # one `$` goes to `docker-compose.yml` escaping
      test: |
        /usr/bin/test $$(
          /usr/bin/curl --fail http://localhost:8000/health/?format=json
          --write-out "%{http_code}" --silent --output /dev/null
        ) -eq 200
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - webnet

  nginx:
    image: afaneor/lct-yakutia-backend
    command:
      - nginx
    ports:
      - "80:8000"
    volumes:
      - django-static:/var/www/django/static
    environment:
      VIRTUAL_HOST: lct.yapa.one
      VIRTUAL_PORT: 8000
      LETSENCRYPT_HOST: lct.yapa.one
    networks:
      - webnet

  llama:
    image: 3x3cut0r/llama-cpp-python:latest
    cap_add:
      - SYS_RESOURCE
    environment:
      MODEL: "model-q5_K.gguf"
      MODEL_DOWNLOAD: "False"
      MODEL_REPO: "local"
      N_CTX: 4096
      MAIN_GPU: 1
    volumes:
      - ./model:/model


# This task is an example of how to extend existing ones:
#   some_worker:
#     <<: *web
#     command: python manage.py worker_process

networks:
  # Network for your internals, use it by default:
  webnet:
  bridge:
    external:
      name: nginx-proxy_default

volumes:
  pgdata:
  django-static:
